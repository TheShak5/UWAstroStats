{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this assignment we learn how to derive a neural network emultor based on the cosmopower emulator (Mancini et al. 2021). Based on this emualtor we perform a PCA data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment you need the install cosmopower yourself https://github.com/alessiospuriomancini/cosmopower or you just use the cosmopower_NN.py module that is provided. However in the latter case you need to install tensorflow: https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import tensorflow as tf\n",
    "from cosmopower_NN import cosmopower_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: load in the all the 10000 model vectors and corresponding parameters that we use to bulit the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KeysView(NpzFile 'data_4_assignment2/parameters.npz' with keys: omega_m, omega_b, As, w),\n",
       " (10000, 900))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = np.load('data_4_assignment2/models.npy') \n",
    "parameters = np.load('data_4_assignment2/parameters.npz')\n",
    "parameters.keys(),models.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the models and parameters in a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = np.arange(0,7000) # select which elements you wanna use for training\n",
    "test_sample = np.arange(7000,10000) # select which elements you wanna use for training\n",
    "\n",
    "train_params = {}\n",
    "for name in parameters.keys():\n",
    "    train_params[name]=list(np.array(parameters[name])[train_sample])\n",
    "    \n",
    "test_params = {}\n",
    "for name in parameters.keys():\n",
    "    test_params[name]=list(np.array(parameters[name])[test_sample])\n",
    "\n",
    "train_features = np.load('data_4_assignment2/models.npy')[train_sample]\n",
    "test_features = np.load('data_4_assignment2/models.npy')[test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the emulator. Reasonable hyper parameres are set already, but test out other settings and comment on what you observe regarding accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialized cosmopower_NN model, \n",
      "mapping 4 input parameters to 900 output modes, \n",
      "using 3 hidden layers, \n",
      "with [4, 5, 6] nodes, respectively. \n",
      "\n",
      "Starting cosmopower_NN training, \n",
      "using 10 per cent of training samples for validation. \n",
      "Performing 2 learning steps, with \n",
      "[0.01, 0.001] learning rates \n",
      "[100, 100] batch sizes \n",
      "[1, 1] gradient accumulation steps \n",
      "[100, 100] patience values \n",
      "[200, 200] max epochs \n",
      "\n",
      "learning rate = 0.01, batch size = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mcp_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtraining_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename_saved_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_4_assignment2/emulator_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# the name of the emulator and where to save it\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# cooling schedule\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The precentage from train sample that is used for the validation\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# the different leanring rates. This need to be adjusted\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# the number for models that are used to adjust the NN parameters\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpatience_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of epoch to wait before decreasing the learning rate if the loss does not improve anymore\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Maxmimal number of epoch before decreasing the learning rate\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patel\\Documents\\GitHub\\UWAstroStats\\projects\\2ptstat-analysis\\assignments\\cosmopower_NN.py:622\u001b[0m, in \u001b[0;36mcosmopower_NN.train\u001b[1;34m(self, training_parameters, training_features, filename_saved_model, validation_split, learning_rates, batch_sizes, gradient_accumulation_steps, patience_values, max_epochs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m theta, feats \u001b[38;5;129;01min\u001b[39;00m training_data:\n\u001b[0;32m    619\u001b[0m \n\u001b[0;32m    620\u001b[0m     \u001b[38;5;66;03m# training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes)\u001b[39;00m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gradient_accumulation_steps[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 622\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step_with_accumulated_gradients(theta, feats, accumulation_steps\u001b[38;5;241m=\u001b[39mgradient_accumulation_steps[i])\n",
      "File \u001b[1;32mc:\\Users\\patel\\Documents\\GitHub\\UWAstroStats\\projects\\2ptstat-analysis\\assignments\\cosmopower_NN.py:463\u001b[0m, in \u001b[0;36mcosmopower_NN.training_step\u001b[1;34m(self, training_parameters, training_features)\u001b[0m\n\u001b[0;32m    460\u001b[0m loss, gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_and_gradients(training_parameters, training_features)\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# apply gradients\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\patel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:268\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m--> 268\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(grads, trainable_variables)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "#These premodifications are not necessary but might improve the accuracy\n",
    "#Here you can try to modify the training features in advance \n",
    "features_modified=train_features\n",
    "\n",
    "cp_nn = cosmopower_NN(parameters=list(parameters.keys()), \n",
    "                    modes=np.linspace(-1,1,train_features.shape[1]), \n",
    "                    n_hidden = [ 4, 5, 6], # This you should modify. In this exmaple we use three layers with 4, 5 and 6 nodes. Is this enough?\n",
    "                    verbose=True, # useful to understand the different steps in initialisation and training\n",
    "                    )\n",
    "\n",
    "device = 'cpu'\n",
    "with tf.device(device):\n",
    "    # train\n",
    "    cp_nn.train(training_parameters=train_params,\n",
    "                training_features=features_modified,\n",
    "                filename_saved_model='data_4_assignment2/emulator_test', # the name of the emulator and where to save it\n",
    "                # cooling schedule\n",
    "                validation_split=0.1, # The precentage from train sample that is used for the validation\n",
    "                learning_rates=[1e-2, 1e-3], # the different leanring rates. This need to be adjusted\n",
    "                batch_sizes=[100, 100], # the number for models that are used to adjust the NN parameters\n",
    "                gradient_accumulation_steps = [1, 1],\n",
    "                patience_values = [100, 100], # Number of epoch to wait before decreasing the learning rate if the loss does not improve anymore\n",
    "                max_epochs = [200,200], # Maxmimal number of epoch before decreasing the learning rate\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy of the emulator, by computing how many predictions are inside 68%, 95%, 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulated_features = cp_nn.predictions_np(test_params)\n",
    "# if you modified the training features you need to convert back the output back the original data vectors\n",
    "\n",
    "diff=(emulated_features/test_features-1) # Measure the relative difference between test sample and the emualted models\n",
    "\n",
    "mean_diff = ... \n",
    "percentiles1 = np.percentile(...) \n",
    "percentiles2 = ... \n",
    "percentiles3 = ...\n",
    "bins=range(diff.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12, 3),dpi=100)\n",
    "plt.fill_between(...,  label = '$99\\%$', alpha=0.8)\n",
    "plt.fill_between(... label = '$95\\%$', alpha = 0.7)\n",
    "plt.fill_between(...  label = '$68\\%$', alpha = 1)\n",
    "plt.plot(bins,mean_diff,'-',color='black')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(frameon=False, fontsize=15, loc='upper right')\n",
    "plt.ylabel(r'$| m^{\\mathrm{emulated}} - m^{\\mathrm{true}}|/  m^{\\mathrm{true}}$', fontsize=15)\n",
    "plt.xlabel(r'vector elements',  fontsize=15)\n",
    "plt.ylim(-0.001,0.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we want perform a Fisher Analysis using the emulator. We want you to compute the covariance of the parameters $\\Theta = \\{\\Omega_\\mathrm{m},w\\}$, which can be estimated by $C(\\Theta) = F^{-1}$, where $$F_{ij}= \\left(\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\right)^{T} C^{-1} \\left(\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\right)$$\n",
    "##### For the partial derivatives we use the five point stencil beam given by $$\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\approx \\frac{-m(\\Theta_i + 2\\ \\Delta \\Theta_i) + 8 \\ m(\\Theta_i +  \\Delta \\Theta_i) - 8 \\ m(\\Theta_i - \\Delta \\Theta_i) + m(\\Theta_i - 2\\ \\Delta \\Theta_i)}{12 \\ \\Delta \\Theta_i }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Om_shift = ... #decide for a reasonable stepsize in Omega_m\n",
    "w_shift = ... #decide for a reasonable stepsize in w\n",
    "\n",
    "# This example compute the model for two different Omega_m values.\n",
    "paramters = {'omega_m':[0.3,0.31,],'w':[-1,-1],'As':[np.mean(test_params['As'])]*2,'omega_b':[np.mean(test_params['omega_b'])]*2}\n",
    "    \n",
    "features_4_div = cp_nn.predictions_np(paramters)\n",
    "# if you modified the training features you need to convert back the output back the original data vectors\n",
    "\n",
    "derivative_Om = ... # compute derivative\n",
    "derivative_w = ... # compute derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So lets compute the Fisher matrix the corresponding covariance matrix of the paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov=np.load('data_4_assignment1/covariance.npy') # We make use of the analy\n",
    "inv_cov = inv(cov)\n",
    "\n",
    "FoM_11 = ...\n",
    "FoM_22 = ...\n",
    "FoM_12 = ...\n",
    "FoM_best = np.array([[FoM_11,FoM_12],[FoM_12,FoM_22]])\n",
    "\n",
    "Cov_parameter = ...\n",
    "Cov_parameter,np.sqrt(np.diag(Cov_parameter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First you need to generate the transformation from our model/data vectors to the PCA elements based on the 10k models. Then you need transform all 100k noisy data vectors from which you should then measure the a cvoavraicen on the PCA elements. Now it should get clear why you had to genereate the 100k multvariate Gaussian random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "N_pca = 900 # Decide how many PCA eigenvalues you want to use\n",
    "models = np.load('data_4_assignment2/models.npy')\n",
    "mean = np.mean(models,axis=0) # for numerical stability you should subtract the mean of each element\n",
    "pca = PCA(n_components=N_pca,svd_solver='full')\n",
    "models_pca = pca.fit_transform(models-mean) # Perform the PCA fitting. Now you can use pca for the transformation\n",
    "\n",
    "#rotate covariance matrix. If you do not how to do that. You can also create 10k random data vectors and tranform each one individual, \n",
    "# and the compute the covariance matrix from them\n",
    "rotation_matrix = pca.components_.T\n",
    "cov_pca = ...\n",
    "\n",
    "plt.imshow(cov_pca/np.outer(np.sqrt(np.diag(cov_pca)),np.sqrt(np.diag(cov_pca))),vmin=-1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase successively the number of PCA elements from which you compute the Fisher matrix. Convince yourself that if you take all possible PCA elements you converge to same contraining power as for original Fisher analysis. How many PCA elements do you need to have 10% and 1% of the constraining power as the original Fisher analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramter_constraints = []\n",
    "for N_pca in np.arange(1,900):\n",
    "    \n",
    "    inv_cov_pca = ... # select the first N_pca elements\n",
    "\n",
    "    features_4_div_pca = pca.transform(...) # transform  the dverative and select the first N_pca elements\n",
    "\n",
    "\n",
    "    derivative_Om_pca = ... # compute derivative\n",
    "    derivative_w_pca = ... # compute derivative\n",
    "\n",
    "    FoM_11 = ...\n",
    "    FoM_22 = ...\n",
    "    FoM_12 = ...\n",
    "    FoM_best_pca = np.array([[FoM_11,FoM_12],[FoM_12,FoM_22]])\n",
    "\n",
    "    Cov_parameter_pca = inv(FoM_best_pca)\n",
    "    paramter_constraints.append(np.sqrt(np.diag(Cov_parameter_pca)))\n",
    "paramter_constraints = np.array(paramter_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the constraints depending on the number of PCA that you have used. How many PCA elements do you need to get 10% and 1% of the maximum constraining power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
