{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this assignment we learn how to derive a neural network emultor based on the cosmopower emulator (Mancini et al. 2021). Based on this emualtor we perform a PCA data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment you need the install cosmopower yourself https://github.com/alessiospuriomancini/cosmopower or you just use the cosmopower_NN.py module that is provided. However in the latter case you need to install tensorflow: https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\patel\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import tensorflow as tf\n",
    "from cosmopower_NN import cosmopower_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: load in the all the 10000 model vectors and corresponding parameters that we use to bulit the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KeysView(NpzFile 'data_4_assignment2/parameters.npz' with keys: omega_m, omega_b, As, w),\n",
       " (10000, 900))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = np.load('data_4_assignment2/models.npy') \n",
    "parameters = np.load('data_4_assignment2/parameters.npz')\n",
    "parameters.keys(),models.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the models and parameters in a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 900)\n",
      "(7000, 900)\n"
     ]
    }
   ],
   "source": [
    "train_sample = np.arange(0,7000) # select which elements you wanna use for training\n",
    "test_sample = np.arange(7000,10000) # select which elements you wanna use for training\n",
    "\n",
    "train_params = {}\n",
    "for name in parameters.keys():\n",
    "    train_params[name]=list(np.array(parameters[name])[train_sample])\n",
    "    \n",
    "test_params = {}\n",
    "for name in parameters.keys():\n",
    "    test_params[name]=list(np.array(parameters[name])[test_sample])\n",
    "\n",
    "train_features = np.load('data_4_assignment2/models.npy')[train_sample]\n",
    "test_features = np.load('data_4_assignment2/models.npy')[test_sample]\n",
    "\n",
    "print(test_features.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'gpu:0' if tf.config.list_physical_devices('GPU') else 'cpu'\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum_feature = np.min(train_features)\n",
    "# maximum_feaure = np.max(train_features - minimum_feature)\n",
    "\n",
    "# features_modified = train_features - minimum_feature\n",
    "# features_modified = features_modified/maximum_feaure\n",
    "\n",
    "# print(np.min(features_modified), np.max(features_modified))\n",
    "\n",
    "# #maximum_feature = np.ma\n",
    "# #features_modified = train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the emulator. Reasonable hyper parameres are set already, but test out other settings and comment on what you observe regarding accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\patel\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "\n",
      "Initialized cosmopower_NN model, \n",
      "mapping 4 input parameters to 900 output modes, \n",
      "using 4 hidden layers, \n",
      "with [250, 350, 450, 512] nodes, respectively. \n",
      "\n",
      "Starting cosmopower_NN training, \n",
      "using 10 per cent of training samples for validation. \n",
      "Performing 5 learning steps, with \n",
      "[0.01, 0.001, 0.0001, 1e-05, 1e-06] learning rates \n",
      "[1024, 1024, 1024, 1024, 1024] batch sizes \n",
      "[1, 1, 1, 1, 1] gradient accumulation steps \n",
      "[100, 100, 100, 100, 100] patience values \n",
      "[1000, 1000, 1000, 1000, 1000] max epochs \n",
      "\n",
      "learning rate = 0.01, batch size = 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 279/1000 [02:43<07:02,  1.71it/s, loss=0.000766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00044644022\n",
      "Model saved.\n",
      "Reached max number of epochs. Validation loss = 0.00044644022\n",
      "Model saved.\n",
      "learning rate = 0.001, batch size = 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/1000 [00:08<09:58,  1.65it/s, loss=0.000106]"
     ]
    }
   ],
   "source": [
    "# Try divings the train_features by the standard deviation\n",
    "\"\"\"features_modified = train_features / np.std(train_features)\"\"\"\n",
    "\n",
    "# Try normalizing everything between 0 and 1. So subtract the minimum and divide by the max\n",
    "\"\"\"minimum_feature = np.min(train_features)\n",
    "maximum_feaure = np.max(train_features - minimum_feature)\n",
    "\n",
    "features_modified = train_features - minimum_feature\n",
    "features_modified = features_modified/maximum_feaure\"\"\"\n",
    "\n",
    "#These premodifications are not necessary but might improve the accuracy\n",
    "#Here you can try to modify the training features in advance \n",
    "features_modified = train_features + 1 \n",
    "\n",
    "cp_nn = cosmopower_NN(parameters=list(parameters.keys()), \n",
    "                    modes=np.linspace(-1,1,train_features.shape[1]), \n",
    "                    n_hidden = [ 250, 350, 450, 512], # This you should modify. In this exmaple we use three layers with 4, 5 and 6 nodes. Is this enough?\n",
    "                    verbose=True, # useful to understand the different steps in initialisation and training\n",
    "                    )\n",
    "\n",
    "device = 'cpu'\n",
    "with tf.device(device):\n",
    "    # train\n",
    "    cp_nn.train(training_parameters=train_params,\n",
    "                training_features=features_modified,\n",
    "                filename_saved_model='data_4_assignment2/emulator_test', # the name of the emulator and where to save it\n",
    "                # cooling schedule\n",
    "                validation_split=0.1, # The precentage from train sample that is used for the validation\n",
    "                \n",
    "                learning_rates=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6], # the different leanring rates. This need to be adjusted\n",
    "                \n",
    "                batch_sizes=[1024, 1024, 1024, 1024, 1024], # the number for models that are used to adjust the NN parameters\n",
    "                # the batch size is the number of samples from the training set to pass through the Neural Network. Generally, lower batch sizes\n",
    "                # leads to less accurate estimates of the gradient which impacts the overal loss and accuracy of the NN\n",
    "                \n",
    "                gradient_accumulation_steps = [1, 1, 1, 1, 1],\n",
    "                \n",
    "                patience_values = [100, 100, 100, 100, 100], # Number of epoch to wait before decreasing the learning rate if the loss does not improve anymore\n",
    "                \n",
    "                max_epochs = [1000, 1000, 1000, 1000, 1000] # Maxmimal number of epoch before decreasing the learning rate\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the version of Tensorflow\n",
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy of the emulator, by computing how many predictions are inside 68%, 95%, 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulated_features = cp_nn.predictions_np(test_params)\n",
    "\n",
    "# if you modified the training features you need to convert back the output back the original data vectors\n",
    "\"\"\"emulated_features = cp_nn.predictions_np(test_params) * np.std(train_features)\"\"\"\n",
    "\n",
    "\"\"\"emulated_features = emulated_features + minimum_feature\n",
    "emulated_features = emulated_features*maximum_feaure\"\"\"\n",
    "\n",
    "# adding 1 as we've added one to the train_features earlier when training the CNN\n",
    "diff=(emulated_features/(test_features+1)-1) # Measure the relative difference between test sample and the emualted models\n",
    "\n",
    "mean_diff = np.mean(diff,0)\n",
    "percentiles1 = np.percentile(diff, [50-34, 50+34],axis=0) \n",
    "percentiles2 = np.percentile(diff, [50-(95/2), 50+(95/2)], axis=0) \n",
    "percentiles3 = np.percentile(diff, [50-(99/2), 50+(99/2)], axis=0)\n",
    "bins=range(diff.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12, 3),dpi=100)\n",
    "plt.fill_between(bins, percentiles3[0], percentiles3[1], label = '$99\\%$', alpha=0.8)\n",
    "plt.fill_between(bins, percentiles2[0], percentiles2[1], label = '$95\\%$', alpha = 0.7)\n",
    "plt.fill_between(bins, percentiles1[0], percentiles1[1], label = '$68\\%$', alpha = 1)\n",
    "plt.plot(bins, mean_diff,'-',color='black')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(frameon=False, fontsize=15, loc='upper right')\n",
    "plt.ylabel(r'$| m^{\\mathrm{emulated}} - m^{\\mathrm{true}}|/  m^{\\mathrm{true}}$', fontsize=15)\n",
    "plt.xlabel(r'vector elements',  fontsize=15)\n",
    "#plt.ylim(-0.001,0.001)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we want perform a Fisher Analysis using the emulator. We want you to compute the covariance of the parameters $\\Theta = \\{\\Omega_\\mathrm{m},w\\}$, which can be estimated by $C(\\Theta) = F^{-1}$, where $$F_{ij}= \\left(\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\right)^{T} C^{-1} \\left(\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\right)$$\n",
    "##### For the partial derivatives we use the five point stencil beam given by $$\\frac{\\partial m(\\Theta)}{\\partial \\Theta_i}\\approx \\frac{-m(\\Theta_i + 2\\ \\Delta \\Theta_i) + 8 \\ m(\\Theta_i +  \\Delta \\Theta_i) - 8 \\ m(\\Theta_i - \\Delta \\Theta_i) + m(\\Theta_i - 2\\ \\Delta \\Theta_i)}{12 \\ \\Delta \\Theta_i }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test_params['As'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Om_shift = 0.001 #decide for a reasonable stepsize in Omega_m\n",
    "w_shift = 0.001 #decide for a reasonable stepsize in w\n",
    "\n",
    "# This example compute the model for two different Omega_m values.\n",
    "#paramters = {'omega_m':[0.3,0.31,],'w':[-1,-1],'As':[np.mean(test_params['As'])]*2,'omega_b':[np.mean(test_params['omega_b'])]*2}\n",
    "\n",
    "mean_omega_m = np.mean(parameters['omega_m'])\n",
    "mean_w = np.mean(parameters['w'])\n",
    "\n",
    "parameters  = {'omega_m':[mean_omega_m + 2*Om_shift, mean_omega_m + Om_shift, mean_omega_m - Om_shift, mean_omega_m - 2*Om_shift],\n",
    "               'w':[mean_w + 2*w_shift, mean_w + w_shift, mean_w - w_shift, mean_w - 2*w_shift],\n",
    "               'As':[np.mean(test_params['As'])]*2,'omega_b':[np.mean(test_params['omega_b'])]*2}\n",
    "#parameters_w = {}\n",
    "\n",
    "# if you modified the training features you need to convert back the output back the original data vectors\n",
    "# Undoing the normalzation performed on the data\n",
    "\n",
    "features = cp_nn.predictions_np(parameters) - 1\n",
    "\n",
    "\n",
    "# print(features_4_div)\n",
    "# #print(feature_omega_m)\n",
    "# print(feature_w)\n",
    "\n",
    "derivative_Om = (-feature_omega_m[0] + 8*feature_omega_m[1] - 8*feature_omega_m[2] + feature_omega_m[3]) / 12*Om_shift # compute derivative\n",
    "derivative_w = (-feature_w[0] + 8*feature_w[1] - 8*feature_w[2] + feature_w[3]) / 12*w_shift # compute derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So lets compute the Fisher matrix the corresponding covariance matrix of the paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov=np.load('data_4_assignment1/covariance.npy') # We make use of the analy\n",
    "inv_cov = inv(cov)\n",
    "\n",
    "FoM_11 = derivative_Om.T @ inv_cov @ derivative_Om \n",
    "FoM_22 = derivative_w.T @ inv_cov @ derivative_w \n",
    "FoM_12 = derivative_Om.T @ inv_cov @ derivative_w \n",
    "FoM_best = np.array([[FoM_11,FoM_12],[FoM_12,FoM_22]])\n",
    "\n",
    "Cov_parameter = ...\n",
    "Cov_parameter,np.sqrt(np.diag(Cov_parameter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First you need to generate the transformation from our model/data vectors to the PCA elements based on the 10k models. Then you need transform all 100k noisy data vectors from which you should then measure the a cvoavraicen on the PCA elements. Now it should get clear why you had to genereate the 100k multvariate Gaussian random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "N_pca = 900 # Decide how many PCA eigenvalues you want to use\n",
    "models = np.load('data_4_assignment2/models.npy')\n",
    "mean = np.mean(models,axis=0) # for numerical stability you should subtract the mean of each element\n",
    "pca = PCA(n_components=N_pca,svd_solver='full')\n",
    "models_pca = pca.fit_transform(models-mean) # Perform the PCA fitting. Now you can use pca for the transformation\n",
    "\n",
    "#rotate covariance matrix. If you do not how to do that. You can also create 10k random data vectors and tranform each one individual, \n",
    "# and the compute the covariance matrix from them\n",
    "rotation_matrix = pca.components_.T\n",
    "cov_pca = ...\n",
    "\n",
    "plt.imshow(cov_pca/np.outer(np.sqrt(np.diag(cov_pca)),np.sqrt(np.diag(cov_pca))),vmin=-1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase successively the number of PCA elements from which you compute the Fisher matrix. Convince yourself that if you take all possible PCA elements you converge to same contraining power as for original Fisher analysis. How many PCA elements do you need to have 10% and 1% of the constraining power as the original Fisher analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramter_constraints = []\n",
    "for N_pca in np.arange(1,900):\n",
    "    \n",
    "    inv_cov_pca = ... # select the first N_pca elements\n",
    "\n",
    "    features_4_div_pca = pca.transform(...) # transform  the dverative and select the first N_pca elements\n",
    "\n",
    "\n",
    "    derivative_Om_pca = ... # compute derivative\n",
    "    derivative_w_pca = ... # compute derivative\n",
    "\n",
    "    FoM_11 = ...\n",
    "    FoM_22 = ...\n",
    "    FoM_12 = ...\n",
    "    FoM_best_pca = np.array([[FoM_11,FoM_12],[FoM_12,FoM_22]])\n",
    "\n",
    "    Cov_parameter_pca = inv(FoM_best_pca)\n",
    "    paramter_constraints.append(np.sqrt(np.diag(Cov_parameter_pca)))\n",
    "paramter_constraints = np.array(paramter_constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the constraints depending on the number of PCA that you have used. How many PCA elements do you need to get 10% and 1% of the maximum constraining power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
